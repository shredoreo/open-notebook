#mysql架构

 

##连接器

缓存

##分析器

词法分析—》语法分析

词法分析：比如："select"这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别成“表名T”，把字符串“ID”识别成“列ID”。

##优化器

- 优化器是在表里面有多个索引的时候，决定使用哪个索引；
- 或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序；

### 优化器的逻辑

依据有下：

- 扫描行数
- 是否使用临时表
- 是否排序

索引的基数

- 一个索引上不同的值越多，这个索引的区分度就越好。而一个索引上不同的值的个数，我们称之为“基数”（cardinality）
- innodb采用“采样统计”的方式，计算出基数。
- 在MySQL中，有两种存储索引统计的方式，可以通过设置参数innodb_stats_persistent的值来选
  择：
  设置为on的时候，表示统计信息会持久化存储。这时，默认的N是20，M是10。
  设置为off的时候，表示统计信息只存储在内存中。这时，默认的N是8，M是16。
- `analyze table t` 命令，可以用来重新统计索引信息。

###解决执行器选错索引的方法：

1. 使用force index xx 语句，强制使用xx索引；
2. 诱导优化器使用较优的索引。如把“order by b limit 1” 改成 “order by b,a limit 1” ，语义的逻辑是相同的。在order by b,a现这种写法，要求按照a排序，就意味着使用这两个索引都需要排序。因此，扫描行数成了影响决策的主要条件，
3. 在有些场景下，我们可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引。
4. 对于由于索引统计信息不准确导致的问题，你可以用analyze table来解决。

##执行器

执行sql语句

 

 

更新操作与查询操作的不同?

更新还是会走上面的流程。但是，需要记录日志。有两种：**redo log（重做日志）**和 **binlog（归档日志）**。

##两阶段提交

###redo log 

- 是inodb特有的日志功能
- 有**crash-safe**特性
- 通过一个*固定大小*的空间来保存操作记录的，配合两个指针：**write pos**是当前记录的位置，**checkpoint**是当前要擦除的位置。若剩余空间不足，则进行硬盘写入操作，以释放。这就是**WAL技术**，WAL的全称是Write-Ahead Logging。

####WAL

​	Write-Ahead Logging，它的关键点就是先写日志，再写磁盘。

- WAL 很重要的一点是**将随机写转换成了顺序写**，而顺序写的性能远远大于随机写。
- 且**延迟刷脏**起到了合并多次修改的效果，避免频繁写数据文件造成的性能问题。



### binlog

- Server提供的日志功能
- 是追加写入的
- 逻辑日志



### redo log 和 binlog 的不同点

| redo log                                             | binlog                                                       |
| ---------------------------------------------------- | ------------------------------------------------------------ |
| 是innodb引擎的日志                                   | 是Server级别的日志                                           |
| 物理日志，<br />记录的是“在某个数据页上做了什么修改” | 逻辑日志，<br />记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ” |
| 循环写入，大小有限                                   | 追加写入                                                     |

### 两阶段提交

浅色框表示是在InnoDB内部执行的，深色框表示是在执行器中执行的。

![](D:\ProgramData\md\update语句执行流程.jpg)

**为什么必须有“两阶段提交”呢？**

这是为了让两份日志之间的逻辑一致。

Mysql 的 redo log 和 binlog 都不可或缺，所以保持 redo log 和 binlog 的事务一致性就很重要，从而就引出了 "两阶段提交" 这个说法。

DML 执行流程 (从数据完全取到内存后开始)：
  数据页到内存中 --> 修改数据 --> 更新数据页 --> 写入 redolog，状态为 prepare--> 写 binlog--> 提交事务，redolog 状态修改为 commit

**在做 Crash recovery 时：**

- binlog 有记录，redolog 状态 commit：正常完成的事务，不需要恢复
- binlog 有记录，redolog 状态 prepare：在 binlog 写完提交事务之前的 crash，恢复操作：提交事务
- binlog 无记录，redolog 状态 prepare：在 binlog 写完之前的 crash，恢复操作：回滚事务
- binlog 无记录，redolog 无记录：在 redolog 写之前 crash，恢复操作：回滚事务



## flush操作

flush就是将内存的数据写入到磁盘的过程。

当内存数据页跟磁盘的数据页不一样时，这个内存页称为**脏页**。内存数据写入到磁盘后，内存与磁盘的数据页内容就一致了，称为**干净页**。

### 触发flush的情形：4

1. redo log 写满，这时候系统会停止所有更新操作，把checkpoint往前推进，redo log留出空间可以继续写。

   checkpoint可不是随便往前修改一下位置就可以的。比如图2中，把checkpoint位置从CP推进到
   CP’，就需要将两个点之间的日志（浅绿色部分），对应的所有脏页都flush到磁盘上。之后，图
   中从write pos到CP’之间就是可以再写入的redo log的区域。

2. 内存不足，需要将一部分的内存页淘汰。若被淘汰的内存页刚好为脏页，那么触发flush。

3. MySQL认为系统“空闲”的时候，适时刷脏。

4. MySQL正常关闭的情况。这时候，MySQL会把内存的脏页都flush到磁盘上，这样下次MySQL启动的时候，就可以直接从磁盘上读数据，启动速度会很快。

分析以上4种场景对性能的影响：

- 第一种要尽量避免，因为会阻塞更新语句。
- 第二种是常态。在innodb中，内存分为3中情况：
- - 还没有使用的；
  - 使用了并且是脏页；
  - 使用了并且是干净页；

刷脏页虽然是常态，但是出现以下这两种情况，都是会明显影响性能的：
1. 一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长；
2. 日志写满，更新全部堵住，写性能跌为0，这种情况对敏感业务来说，是不能接受的。

### InnoDB刷脏页的控制策略

一、innodb_io_capacity

首先，你要正确地告诉InnoDB所在主机的IO能力，这样InnoDB才能知道需要全力刷脏页的时
候，可以刷多快。就是`innodb_io_capacity`	,这个值我建议你设置成磁盘的IOPS



二、如果你来设计策略控制刷脏页的速度，会参考哪些因素呢？

InnoDB的刷盘速度就是要参考这两个因素：一个是脏页比例，一个是redo log写盘速度。

- 参数`innodb_max_dirty_pages_pct`是脏页比例上限，默认值是75%。InnoDB会根据当前的脏页比例（假设为M），算出一个范围在0到100之间的数字，
- InnoDB每次写入的日志都有一个序号，当前写入的序号跟checkpoint对应的序号之间的差值，我们假设为N。InnoDB会根据这个N算出一个范围在0到100之间的数字
- 根据上述算得的F1(M)和F2(N)两个值，取其中较大的值记为R，之后引擎就可以按照`innodb_io_capacity`定义的能力乘以R%来控制刷脏页的速度。



三、，innodb_flush_neighbors

MySQL中的一个机制，可能让你的查询会更慢：在准备刷一个脏页的时候，如果这个数据页旁边的数据页刚好是脏页，就会把这个“邻居”也带着一起刷掉；而且这个把“邻居”拖下水的逻辑还可以继续蔓延，也就是对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷。

在InnoDB中，`innodb_flush_neighbors` 参数就是用来控制这个行为的，值为1的时候会有上述的“连坐”机制，值为0时表示不找邻居，自己刷自己的。

对于机械硬盘，由于IOPS较低，这个参数可以设大；对于SSD建议为0，减少刷脏量，提升响应速度。

#事务

## 事务的隔离级别

SQL标准的事务隔离级别包括：读未提交（read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（serializable ）。

- 读未提交：一个事务还没提交时，它做的变更就能被别的事务看到。
- 读提交：一个事务提交之后，它做的变更才会被其他事务看到。
- 可重复读：一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。
- 串行化：顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。



## 隔离级别的实现

在实现上，数据库里面会创建一个**视图**，访问的时候以视图的逻辑结果为准。

在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。

在“读提交”隔离级别下，这个视图是在每个SQL语句开始执行的时候创建的。这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；

而“串行化”隔离级别下直接用加锁的方式来避免并行访问。



每条记录在更新的时候，会记录一个回滚日志（undo log），记录上的最新值都可以通过回滚操作得到原值。

![](C:\Users\shred\Desktop\59ba421ae9712\03.jpg)



## 事务的启动方式

1. 显式启动事务语句， begin 或 start transaction。配套的提交语句是commit，回滚语句是rollback。
2. set autocommit=0，这个命令会将这个线程的自动提交关掉。**意味着如果你只执行一个select语句，这个事务就启动了，而且并不会自动提交。**这个事务持续存在直到你主动执行commit 或 rollback 语句，或者断开连接。

建议你总是使用**set autocommit=1, 通过显式语句的方式来启动事务**。

在autocommit为1的情况下，用begin显式启动的事务，如果执行commit则提交事务。如果执行**commit work and chain**，则是提交事务并自动启动下一个事务，这样也省去了再次执行begin语句的开销。



## 一致性视图工作原理

通过之前的文章我们知道，在可重复读隔离级别下，事务开始前会创建一个一致性视图。下面我们来详细说明一下这个一致性视图的工作原理。

在 InnoDB 引擎中，每个事务都有一个唯一的 ID，就是 transaction id。它是在事务开始的时候向系统申请的，是严格按顺序递增的。我们知道，每个数据行都是有多个版本的。每一次的事务更新都会有一个新的版本，并且每个版本都有对应的 transaction id（row trx_id）。

![img](https://img-blog.csdnimg.cn/20190223104309849.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1eGlhbjkw,size_16,color_FFFFFF,t_70)

## 如何避免长连接？



# 索引

innoDB使用B+树作为索引结构

## 常见索引模型

### 哈希索引

哈希表是一种以键-值（key-value）存储数据的结构，我们只要输入待查找的值即key，就可以找
到其对应的值即Value。利用数组实现。

但是会发生哈希冲突，常用链表法来解决。

使用场景：只适用于等值查询。在进行区间查询的时候需要遍历所有数据。

### 有序数组

适用于等值查询和区间查询。查询效率高，一般使用二分法。但是更新的时候需要移动后面的数据。

使用场景：只适用于静态存储引擎，比如你要保存的是2017年某个城市的所有人口信息，这类不会再修改的数据。

###  搜索树

二叉搜索树的特点是：每个节点的左儿子小于父节点，父节点又小于右儿子。

二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上。树太高了。

为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不应该使用二叉树，而是要使用“N叉”树。这里，“N叉”树中的“N”取决于数据块的大小。

N叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中了。

## InnoDB使用的索引

使用的是B+树 。

主键索引，也称为聚簇索引，它的叶子节点保存的是整行数据。

非主键索引，称为二级索引，叶子节点的内容是主键的值。所以会使用主键进行回表。

## 索引的维护

B+树为了维护索引有序性，在插入新值的时候需要做必要的维护。

- 需要逻辑上挪动后面的数据，空出位置。
- 也可能出现**页分裂**的情况。即申请一个新页，将数据移动过去，这时候整体的空间利用率会降低一半。
- 若删除的过多，会出现**页合并**。

分裂和合并都会影响性能。

### 建议使用自增主键作为索引

- **性能角度**：插入数据都是追加的，不涉及挪动和叶子节点的分裂。

- **空间角度**：若存在其他索引，他们的叶子节点保存的是主键值，主键长度越小则空间占用越低。

  

有没有什么场景适合用业务字段直接做主键的呢？还是有的，典型的KV场景。比如，有些业务的场景需求是这样的：

1.	只有一个索引；
2.	该索引必须是唯一索引。

## 覆盖索引

通过索引查数据，若需要查找的字段已经保存在当前索引的叶子上，那么直接返回值，不需要回表。

也就是通过冗余，创建联合索引。

当 sql 语句的所求查询字段（select 列）和查询条件字段（where 子句）全都包含在一个索引中，可以直接使用索引查询而不需要回表。这就是覆盖索引，通过使用覆盖索引，可以减少搜索树的次数，是常用的性能优化手段。

### 最左前缀原则

B+树这种索引结构，可以使用最左前缀原则，来定位记录。

索引的复用能力。因为可以支持最左前缀，所以当已经有了(a,b)这个联合索引后，一般就不需要单独在a上建立索引了。因此，第一原则就是，通过调整顺序，可以少维护一个索引，那么优先使用这种顺序。

不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。**这个最左前缀可以是联合索引的最左N个字段，也可以是字符串索引的最左M个字符。**	

那么，如果既有联合查询，又有基于a、b各自的查询呢？查询条件里面只有b的语句，是无法使用(a,b)这个联合索引的，这时候你不得不维护另外一个索引，也就是说你需要同时维护(a,b)、(b) 这两个索引。

第二个考虑的就是空间了。

要根据姓名查年龄，可以使用（姓名，年龄），加上一个（年龄），因为年龄的长度小于姓名，建立额外的索引少消耗一些空间。

### 索引下推优化

- 通过修改系统变量 optimizer_switch 的 index_condition_pushdown 标志来控制

- innodb 引擎的表，索引下推只能用于二级索引。

  innodb 的主键索引树叶子结点上保存的是全行数据，所以这个时候索引下推并不会起到减少查询全行数据的效果。

- 索引下推一般可用于所求查询字段（select 列）不是 / 不全是联合索引的字段，查询条件为多条件查询且查询条件子句（where/order by）字段全是联合索引。

索引下推就是指，在使用索引时，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。



## 普通索引和唯一索引

### 查询操作

如select id from T where k=5

普通索引：查询会从根节点开始，查找k=5，找到第一条记录，还会继续往下找，k=5的。

唯一索引：由于定义了唯一性，找到一条记录直接返回。

可以看到，性能上没有太大区别。

### 更新操作

普通索引：如果数据所在页在内存中，那么直接在内存更改，并将操作记录到`change buffer` 中。若不在内存，也记录到`change buffer`中，等下次需要读取这个数据的时候，再将他们写入到磁盘。

唯一索引：更新时要保证唯一性，若数据页不在内存中，就要将数据页读取过来，进行判断和更新。不会用到change buffer。

普通索引：若数据页不在内存，将操作记录到change buffer 中，等下次读取数据页的时候再更新内存。

可以看出，普通索引能利用change buffer 减少随机IO访问。

### change buffer

当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InooDB会将这些更新操作缓存在change buffer中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个**数据页**的时候，将数据页读入内存，然后执行change buffer中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。

change buffer用的是`buffer pool`里的内存，因此不能无限增大。change buffer的大小，可以通过参数` innodb_change_buffer_max_size`来动态设置。这个参数设置为50的时候，表示change buffer的大小最多只能占用buffer pool的50%。

- change buffer也能做持久化的。在`merge`操作时持久化。

  merge就是将change buffer 的操作应用到磁盘的原数据页中，得到最新结果的过程。

  merge触发的情形：1.访问数据页；2.数据库定期执行；3.数据库正常关闭时，执行。

- 使用场景：

  对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时change buffer的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。

  假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在change buffer，但之后由于马上要访问这个数据页，会立即触发merge过程。这样随机访问IO的次数不会减少，反而增加了change buffer的维护代价。

###change buffer 与 redo log

对于更新语句

```
insert into t(id,k) values(id1,k1),(id2,k2);
```

这里，我们假设当前k索引树的状态，查找到位置后，k1所在的数据页在内存(InnoDB buffer
pool)中，k2所在的数据页不在内存中。

![](D:\ProgramData\md\带change buffer的更新过程.jpg)

分析这条更新语句，你会发现它涉及了四个部分：内存、redo log（ib_log_fileX）、 数据表空间（t.ibd）、系统表空间（ibdata1）。
这条更新语句做了如下的操作（按照图中的数字顺序）：

1. Page 1在内存中，直接更新内存；
2. Page 2没有在内存中，就在内存的change buffer区域，记录下“我要往Page 2插入一行”这个
信息
备注：这里，你可以再回顾下第2篇文章《日志系统：一条SQL更新语句是如何执行的？》中
的相关内容。
mysql> insert into t(id,k) values(id1,k1),(id2,k2);
3. 将上述两个动作记入redo log中（图中3和4）。

如果读语句发生在更新语句后不久，内存中的数据都还在，那么此时的这两个读操作就与系统表
空间（ibdata1）和 redo log（ib_log_fileX）无关了。所以，我在图中就没画出这两部分。![](D:\ProgramData\md\带change buffer的读过程.jpg)
图3 带change buffer的读过程
从图中可以看到：

1. 读Page 1的时候，直接从内存返回。有几位同学在前面文章的评论中问到，WAL之后如果
读数据，是不是一定要读盘，是不是一定要从redo log里面把数据更新以后才可以返回？其
实是不用的。你可以看一下图3的这个状态，虽然磁盘上还是之前的数据，但是这里直接从
内存返回结果，结果是正确的。
2. 要读Page 2的时候，需要把Page 2从磁盘读入内存中，然后应用change buffer里面的操作
   日志，生成一个正确的版本并返回结果。


如果要简单地对比这两个机制在提升更新性能上的收益的话， **redo log 主要节省的是随机写磁盘的IO消耗（转成顺序写），而change buffer主要节省的则是随机读磁盘的IO消耗。**

### merge

merge的执行流程是这样的：
1. 从磁盘读入数据页到内存（老版本的数据页）；

2. 从change buffer里找出这个数据页的change buffer 记录(可能有多个），依次应用，得到新
  版数据页；

3. 写redo log。这个redo log包含了数据的变更和change buffer的变更。

  到这里merge过程就结束了。这时候，数据页和内存中change buffer对应的磁盘位置都还没有修
  改，属于脏页，之后各自刷回自己的物理数据，就是另外一个过程了。

## 前缀索引

MySQL是支持前缀索引的，也就是说，你可以定义字符串的一部分作为索引。默认地，如果你创建索引的语句不指定前缀长度，那么索引就会包含整个字符串。

优势： 占用空间相比于普通索引要小。

劣势：可能会增加额外的扫描行数。

> 接下来，我们再看看下面这个语句，在这两个索引定义下分别是怎么执行的。
>
> select id,name,email from SUser where email='zhangssxyz@xxx.com';
>
> 如果使用的是index1（即email整个字符串的索引结构），执行顺序是这样的：
>
> 1. 从index1索引树找到满足索引值是’zhangssxyz@xxx.com’的这条记录，取得ID2的值；
> 2. 到主键上查到主键值是ID2的行，判断email的值是正确的，将这行记录加入结果集；
> 3. 取index1索引树上刚刚查到的位置的下一条记录，发现已经不满足
> email='zhangssxyz@xxx.com’的条件了，循环结束。
>
> 如果使用的是index2（即email(6)索引结构），执行顺序是这样的：
> 1. 从index2索引树找到满足索引值是’zhangs’的记录，找到的第一个是ID1；
> 2. 到主键上查到主键值是ID1的行，判断出email的值不是’zhangssxyz@xxx.com’，这行记录丢
> 弃；
> 3. 取index2上刚刚查到的位置的下一条记录，发现仍然是’zhangs’，取出ID2，再到ID索引上取
> 整行然后判断，这次值对了，将这行记录加入结果集；
> 4. 重复上一步，直到在idxe2上取到的值不是’zhangs’时，循环结束。
> 在这个过程中，要回主键索引取4次数据，也就是扫描了4行。



也就是说使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。

可以通过加长前缀索引的长度，来增加区分度，减少扫描的行数。我们在建立索引时关注的是区分度，区分度越高越好。因为区分度越高，意味着重复的键值越少。因此，我们可以通过统计索引上有多少个不同的值来判断要使用多长的前缀。



### 前缀索引对覆盖索引的影响

使用前缀索引就用不上覆盖索引对查询性能的优化了，这也是你在选择是否使用前缀索引时需要考虑的一个因素。

> 你先来看看这个SQL语句：select id,email from SUser where email='zhangssxyz@xxx.com';
> 与前面例子中的SQL语句
> 相比，这个语句只要求返回id和email字段。
> 所以，如果使用index1（即email整个字符串的索引结构）的话，可以利用覆盖索引，从index1查
> 到结果后直接就返回了，不需要回到ID索引再去查一次。而如果使用index2（即email(6)索引结
> 构）的话，就不得不回到ID索引再去判断email字段的值。
> 即使你将index2的定义修改为email(18)的前缀索引，这时候虽然index2已经包含了所有的信息，
> 但InnoDB还是要回到id索引再查一下，因为系统并不确定前缀索引的定义是否截断了完整信
> 息。

### 对字符串建立前缀索引

对于类似于邮箱这样的字段来说，使用前缀索引的效果可能还不错。但是，遇到前缀的区分度不够好的情况时，我们要怎么办呢？
比如，我们国家的身份证号，一共18位，其中前6位是地址码，所以同一个县的人的身份证号前6位一般会是相同的。

可能会选取长度更大的前缀索引

但是，**索引选取的越长，占用的磁盘空间就越大，相同的数据页能放下的索引值就越少，搜索的效率也就会越低。**

何解？

法一：使用倒叙保存，建立索引，每次查询时，再倒回来。

法二：使用hash字段。在原表上建一个额外的整数字段，保存校验码。同时，使用这个字段建立索引。
	然后每次插入新记录的时候，都同时用crc32()这个函数得到校验码填到这个新字段。由于校验码可能存在冲突，也就是说两个不同的身份证号通过crc32()函数得到的结果可能是相同的，所以你的查询语句where部分要判断id_card的值是否精确相同。

> 它们的区别，主要体现在以下三个方面：
> 1. 从占用的额外空间来看，倒序存储方式在主键索引上，不会消耗额外的存储空间，而hash字
> 段方法需要增加一个字段。当然，倒序存储方式使用4个字节的前缀长度应该是不够的，如
> 果再长一点，这个消耗跟额外这个hash字段也差不多抵消了。
> 2. 在CPU消耗方面，倒序方式每次写和读的时候，都需要额外调用一次reverse函数，而hash
> 字段的方式需要额外调用一次crc32()函数。如果只从这两个函数的计算复杂度来看的
> 话，reverse函数额外消耗的CPU资源会更小些。
> 3. 从查询效率上看，使用hash字段方式的查询性能相对更稳定一些。因为crc32算出来的值虽
> 然有冲突的概率，但是概率非常小，可以认为每次查询的平均扫描行数接近1。而倒序存储
> 方式毕竟还是用的前缀索引的方式，也就是说还是会增加扫描行数。

字符串字段创建索引的场景。我们来回顾一下，你可以使用的
方式有：
1. 直接创建完整索引，这样可能比较占用空间；
2. 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引；
3. 倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题；
4. 创建hash字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支
持范围扫描。

# 锁机制

- **读锁**是共享的，是异步的，是相互不阻塞的，高并发下，多个用户同时读取数据库中的同一个资源，可以相互不干扰；

- **写锁**是排他的，同步的，是会阻塞其他的用户的写锁与读锁，只有这样才能保证，在一个时间段内，只有一个用户在对数据库进行操作，从而防止了其他用户访问到了正在修改的数据。

互斥关系

- 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。
- 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。

## 全局锁

全局锁是对整个数据库的锁，

- MySQL提供了一个加全局读锁的方法，命令是**Flush tables with read lock (FTWRL)**。这个命令会让其他线程的一下语句阻塞：

  数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。

这个功能用于做全库逻辑备份。

也可以通过事务的方式，在可重复读级别下，开启事务，拿到一个一致性视图。

mysqldump使用参数–single-transaction的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于MVCC的支持，这个过程中数据是可以正常更新的。

但是，对于不支持事务的引擎，只能使用ftwrl。

也有另一种方法，但是风险高。就是set global readonly=true。

> set global readonly=true的风险：
>
> 一是，在有些系统中，readonly的值会被用来做其他逻辑，比如用来判断一个库是主库还是备
> 库。因此，修改global变量的方式影响面更大，我不建议你使用。
> 二是，在异常处理机制上有差异。如果执行FTWRL命令之后由于客户端发生异常断开，那么
> MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为
> readonly之后，如果客户端发生异常，则数据库就会一直保持readonly状态，这样会导致整个
> 库长时间处于不可写状态，风险较高。

## 表级锁

表级锁有两种，1、表锁。2、元数据锁（MDL）。

### 表级锁

- 显式使用。
- 表锁的语法是 lock tables … read/write。与FTWRL类似，可以用unlock tables主动释放锁，也可以在客户端断开的时候自动释放。
- 需要注意，lock tables语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。

### MDL（metadata lock)

- 不需要显式使用，需要时自动加。MDL会在事务提交才释放。
- 在使用DML语句时，自动加上DML读锁；在使用DDL时，自动加DML写锁。
- 但是，对写锁的申请会阻塞后面发生的读锁申请。

## 行锁

### 两阶段锁协议

两阶段锁：即**行锁会在需要更改这个行的时候获取，在事务提交后释放。**

启示：如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。减少锁占用的时间。

### 死锁和死锁检测

死锁的产生：

![死锁的产生](D:\ProgramData\md\死锁的产生.jpg)

解决策略：

1. 设置锁请求超时。超过等待时间自动释放持有的锁。

   通过innodb_lock_wait_timeout参数设置。

   缺点：

   1. 默认时间为50s，时间太长，不合理。
   2. 时间若太短，容易引起误判。比如，单纯的锁等待，误伤了。

2. 死锁检测。发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。

   nnodb_deadlock_detect的默认值本身就是on。

   缺点：

   热点行的并发量高，处理死锁链条很慢。消耗cpu资源，导致事务处理能力下降。

### 怎么解决由这种热点行更新导致的性能问题呢？

#### 策略1：关掉死锁检测。

在确保业务不会出现死锁的情况下，关掉死锁检测。

#### 策略2：控制并发度。

在服务端控制，基本思路就是，对于相同行的更新，在进入引擎之前排队。这样在InnoDB内部就不会有大量的死锁检测工作了。这需要修改mysql

#### 策略3：优化表的设计。

将一行改为多行，那么就有多个锁可以用。

> 以影院账户为例，可以考虑放在多
> 条记录上，比如10个记录，影院的账户总额等于这10个记录的值的总和。这样每次要给影院账
> 户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的1/10，可以减少锁等
> 待个数，也就减少了死锁检测的CPU消耗。

**减少死锁的主要方向，就是控制访问相同资源的并发事务量。**



#事务隔离级别详解

## 事务启动时机

begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作InnoDB表的语句，事务才真正启动。

如果你想要马上启动一个事务，可以使用start transaction with consistent snapshot 这个命令。

## MVCC中的快照

在可重复读级别，事务在启动时就拍了一个快照，即一致性视图，它是基于整库的。

实现原理：

- innodb每个事务都有唯一的事务id，在事务开始的时候申请，并且按申请顺序严格递增。

- 每行数据都有多个版本，每次事务更新这个行时，都会把transaction id赋值给这个版本的事务id，记为row trx_id，表示更新该版本的事务。同时旧的版本信息也保留。如下图![](D:\ProgramData\md\行状态变更.jpg)

- 语句更新会生成undo log（回滚日志）吗？那么，undo log在哪呢？

  图2中的三个虚线箭头，就是undo log；而V1、V2、V3并不是物理上真实存在的，而
  是每次需要的时候根据当前版本和undo log计算出来的。

- InnoDB为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活
  跃”的所有事务ID。“活跃”指的就是，启动了但还没提交。

- 事务id最小的，为低水位；最大值加1，为高水位。**这个视图数组和高水位，就组成了当前事务的一致性视图。**如下图

  ![](D:\ProgramData\md\数据版本可见性规则.jpg)

- 这样，对于当前事务的启动瞬间来说，一个数据版本的row trx_id，有以下几种可能：
  1. 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是
  可见的；
  2. 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的；
  3. 如果落在黄色部分，那就包括两种情况
  a. 若 row trx_id在数组中，表示这个版本是由还没提交的事务生成的，不可见；
  b. 若 row trx_id不在数组中，表示这个版本是已经提交了的事务生成的，可见。

总结：**InnoDB利用了“所有数据都有多个版本”的这个特性，实现了“秒级创建快照”的能力。**

**一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况：**

1. 版本未提交，不可见；
2. 版本已提交，但是是在视图创建后提交的，不可见；
3. 版本已提交，而且是在视图创建前提交的，可见。

## 更新逻辑

### 当前读（current read）

**在更新数据时，update，会先读后写**，这个读，只能读当前的值，称为更新读。

其实，除了update语句外，select语句如果加锁，也是当前读。

加锁方法：加上lock in share mode 或 for update，也即读锁（S锁，共享锁），写锁（X锁，排他锁）。

```
mysql> select k from t where id=1 lock in share mode;
mysql> select k from t where id=1 for update;
```



# 数据库语言

##一、DML

- DML（data manipulation language）数据操纵语言：

　　　　就是我们最经常用到的 SELECT、UPDATE、INSERT、DELETE。 主要用来对数据库的数据进行一些操作。

```
SELECT 列名称 FROM 表名称
UPDATE 表名称 SET 列名称 = 新值 WHERE 列名称 = 某值
INSERT INTO table_name (列1, 列2,...) VALUES (值1, 值2,....)
DELETE FROM 表名称 WHERE 列名称 = 值
```

##二、DDL

- DDL（data definition language）数据库定义语言：

　　　　其实就是我们在创建表的时候用到的一些 sql，比如说：CREATE、ALTER、DROP 等。DDL 主要是用在定义或改变表的结构，数据类型，表之间的链接和约束等初始化工作上

[![img](http://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
CREATE TABLE 表名称
(
列名称1 数据类型,
列名称2 数据类型,
列名称3 数据类型,
....
)

ALTER TABLE table_name
ALTER COLUMN column_name datatype

DROP TABLE 表名称
DROP DATABASE 数据库名称
```

[![img](http://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

## 三、DCL

- DCL（Data Control Language）数据库控制语言：

　　　　是用来设置或更改数据库用户或角色权限的语句，包括（grant,deny,revoke 等）语句。这个比较少用到。

在公司呢一般情况下我们用到的是 DDL、DML 这两种。

# 主从数据库

## 一、主从复制

主从复制就是用来建立一个与主数据库完全相同的数据库，即从数据库；主库一般是准实时的业务数据库。

## 二、主从复制的好处

1. 做热备，主库故障时可以切换到从库。
2. 架构的扩展。业务量越来越大，I/O访问频率过高，单机无法满足，此时做多库的存储，降低磁盘I/O访问的频率，提高单个机器的I/O性能。
3. 实现读写分离，使数据库能支持更大的并发量。在报表中尤其重要。由于部分报表sql语句非常的慢，导致锁表，影响前台服务。如果前台使用master，报表使用slave，那么报表sql将不会造成前台锁，保证了前台速度。

##三、主从复制的原理

1. 利用binlog 文件，里面记录了说有sql语句。
2. 需要将binlog复制到从从库中，然后，从库在relay-log中执行这些sql。
3. 具体需要三个线程
   1. 主库 开启IO线程，用于输出binlog
   2. 从库 IO线程，用于接收主库的binlog，将sql写入到relay log
   3. 从库sql线程，用于读取relaylog，执行sql

![wps1647.tmp](https://images2017.cnblogs.com/blog/1228077/201712/1228077-20171222172528412-149389594.png)



# 重建表

##innodb_file_per_table

1. 这个参数设置为OFF表示的是，表的数据放在系统共享表空间，也就是跟数据字典放在一
起；
2. 这个参数设置为ON表示的是，每个InnoDB表数据存储在一个以 .ibd为后缀的文件中。

在mysql8中，这个参数值默认是on，而且也推荐是on，因为这样使用文件保存数据，更方便管理，而且drop table能直接删掉文件。

- 在删除行数据时，innodb会把该行标记为删除，这个标记的位置可能会被复用，但是插入的数据需要符合条件，刚好在这个位置插入。

- 删除数据页，整个数据页标记为复用。

  这些可以复用，而没有被使用的空间，看起来就像是“**空洞**”。

- 不止是删除数据会造成空洞，插入数据也会。比如，插入时造成页分裂，比如出现空洞。

- 另外，更新索引上的值，可以理解为删除一个旧的值，再插入一个新值。不难理解，这也是会造
  成空洞的。

**如果能够把这些空洞去掉，就能达到收缩表空间的目的。**
而重建表，就可以达到这样的目的。

## 重建表

用alter table A engine=InnoDB命令来重建表。

- 在MySQL 5.5版本之前，这个命令的执行流程跟我们前面描述的差不多，区别只是这个临时表B不需要你自己创建，MySQL会自动完成转存数据、交换表名、删除旧表的操作。

> 花时间最多的步骤是往临时表插入数据的过程，如果在这个过程中，有新的数据要写入到表A的话，就会造成数据丢失。
>
> 因此，在整个DDL过程中，表A中不能有更新。也就是说，这个DDL不是Online的。
>
> ![](D:\ProgramData\md\改锁表DDL.jpg)



- 在MySQL 5.6版本开始引入的Online DDL，对这个操作流程做了优化。

> Online DDL之后，重建表的流程：
> 1. 建立一个临时文件，扫描表A主键的所有数据页；
> 2. 用数据页中表A的记录生成B+树，存储到临时文件中；
> 3. 生成临时文件的过程中，将所有对A的操作记录在一个日志文件（row log）中，对应的是图
>    中state2的状态；
> 4. 临时文件生成后，将日志文件中的操作应用到临时文件，得到一个逻辑数据上与表A相同的
>    数据文件，对应的就是图中state3的状态；
> 5. 用临时文件替换表A的数据文件。
>
> ![](D:\ProgramData\md\OnlineDDL.jpg)

**DDL之前是要拿MDL写锁的，这样还能叫Online DDL吗？**

> alter语句在启动的时候需要获取MDL写锁，但是这个写锁在真正拷贝数据之前就**退化成读锁**了。
>
> 为什么要退化呢？为了实现Online，MDL读锁不会阻塞增删改操作。
> 那为什么不干脆直接解锁呢？为了保护自己，禁止其他线程对这个表同时做DDL。

## Online 和 inplace

在重建表的过程中，数据是放在temp_file 文件中的，整个DDL过程在innodb内部完成，在**server端并没有将数据挪动到临时表**，而是一个**原地操作**。

我们重建表的这个语句alter table t engine=InnoDB，其实隐含的意思是：

```
alter table t engine=innodb,ALGORITHM=inplace;
```

跟inplace对应的就是拷贝表的方式了，用法是：

```
alter table t engine=innodb,ALGORITHM=copy;
```

当你使用ALGORITHM=copy的时候，表示的是强制拷贝表，对应的流程就是图3的操作过程。
但我这样说你可能会觉得，inplace跟Online是不是就是一个意思？
其实不是的。如果说这两个逻辑之间的关系是什么的话，可以概括为：

1. DDL过程如果是Online的，就一定是inplace的；
2. 反过来未必，也就是说inplace的DDL，有可能不是Online的。截止到MySQL 8.0，添加全文
索引（FULLTEXT index）和空间索引(SPATIAL index)就属于这种情况。


##optimizetable、analyze table和alter table的区别

1. alter table t engine = InnoDB（也就是recreate）默认的就是上面图4的流程了；
2. analyze table t 其实不是重建表，只是对表的索引信息做重新统计，没有修改数据，这个过程中加了MDL读锁；
3. optimize table t 等于recreate+analyze。

